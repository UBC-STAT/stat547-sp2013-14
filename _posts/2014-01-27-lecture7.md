---
layout: post
title: "Lecture 7: Related topics in computational statistics"
category: 'Lecture'
---

Instructor: Alexandre Bouchard-C&ocirc;t&eacute;

Editor: TBA

### Related topics in computational statistics

By now, you should all be familiar with hard inference problems of the form
\begin{eqnarray}\label{eq:problem}
\bar \pi(x) = \frac{\pi(x)}{Z},
\end{eqnarray}
where the goal is to compute an expectation with respect to the density $\bar \pi$ and/or estimate $Z = \normof{\pi}$. For simplicity, for the first part of today we will assume the space is discrete, so you can think of $\bar \pi$ as a p.m.f.

Our running example will be the posterior of seatings under a CRP.

We will call $\phi$ the function we want to take expectation of:

\begin{eqnarray}
\bar \pi \phi = \sum\_x \phi(x) \bar \pi(x).
\end{eqnarray}

Example of $\phi$?

MCMC is one solution but suffers from some problems:

- Fundamentally sequential, making it harder to adapt to parallel architectures (GPUs, multi-processors, clusters)
- Reversible moves can be difficult to construct in some situations (e.g. requiring reversible jumps)
- The normalization constant $Z$ is non-trivial to estimate from the MCMC output
- Non-trivial to analyze

Today we will discuss other approaches to Equation~(\ref{eq:problem}):

- Sequential Monte Carlo (SMC)
  - PMCMC
  - Generalized view (if time permits)
- Exact propagation of messages
- HMC (if time permits)

#### Sequential Monte Carlo (SMC)

**Background:** The foundation of SMC is importance sampling (IS). IS is a way of building a random measure $\tilde \pi^{(K)}$ such that:

- $\pi^{(K)}$ approximates $\pi$,
- it is easy to compute expectations and normalization of $\pi^{(K)}$,
- these two approximations converge to the true value as $k \to \infty$.

IS works by sampling some samples $X\_i$ according to a proposal with density $q$, and building a random measure with atoms at the $X\_i$s

\begin{eqnarray}
\pi^{(K)} = \sum\_k w^{(k)} \delta\_{X^{(k)}}
\end{eqnarray}

where the random weights are given by $w^{(k)} = w(X^{(k)})$, with: 

\begin{eqnarray}
w(x) = \frac{\pi(x)}{q(x)}
\end{eqnarray}

Note:

1. the normalization $\normof{\pi^{(K)}}$ of this approximation is easy to compute (what is it?)
2. expectations with respect to $\bar \pi^{(K)}$ are easy to compute (what do they look like?)

As long as the support of $q$ is no smaller than the support of $\pi$, we have convergence almost sure (a.s.) of the estimates 1 and 2 above to their targets $Z$ and $\bar \pi \phi$. The key to proving this argument (exercise) is a change of measure:

\begin{eqnarray}
Z &=& \sum\_x \pi(x) \\\\
&=& \sum\_x \left(\frac{\pi(x)}{q(x)}\right) q(x),
\end{eqnarray}

which can be interpreted as $\E[w(X)]$ if $X \sim q$. By the LLN, the approximation

\begin{eqnarray}
\frac{1}{K} \sum_k w\left(X^{(k)}\right)
\end{eqnarray}

will therefore converge to $Z$ if the $X^{(k)}$ are iid samples from $q$.

**Sequential Importance Sampling (SIS):** In the CRP case, it is more natural to write $q$ as a product of conditionals:

\begin{eqnarray}
q(x) = \prod\_n q(x\_{1:n} | x\_{1:n-1}),
\end{eqnarray}

where $x\_{1:n}$ is the seating arrangement of customers 1 through $n$.

We can modify the IS to exploit this structure. This is useful for example in an online setting, where new datapoints come in dynamically.

To do this, we need to consider not only one target distribution $\pi$, but instead several target distributions given by

\begin{eqnarray}
\bar \pi\_n = \frac{\pi\_n}{Z\_n},
\end{eqnarray}

where $\bar \pi\_n$ can be interpreted as the posterior distribution on seating arrangement given datapoints 1 through $n$.

With this in hand, we now simply use a telescoping product to rewrite the weight as a sequence of *weight updates*:

\begin{eqnarray}
\pi\_n &=& \left( \frac{\pi\_1}{\pi\_0} \right) \left( \frac{\pi\_2}{\pi\_1} \right) \dots \left( \frac{\pi\_n}{\pi\_{n-1}} \right) \\\\
w\_n(x\_{1:n}) &=& \frac{\pi\_n(x\_{1:n})}{\pi\_{n-1}(x\_{1:n-1})} \frac{1}{q(x\_{1:n}|x\_{1:n-1})} \\\\
w &=& \prod\_n w\_n \\\\
\end{eqnarray}

where $\pi\_0 \equiv 1$. 

---

**Example:** weight update for the CRP posterior.

A simple proposal consists in using the CRP prior probabilities, $q(x\_{1:n}|x\_{1:n-1}) = \crp(\part(x\_{1:n})|\part(x\_{1:n-1}))$. If we let $\part = \part(x\_{1:n-1})$ and $\part' = \part(x\_{1:n})$, we get:

\begin{eqnarray}
w\_n(x\_{1:n}) &=& \frac{\CRP(\part') \left( \prod\_{B' \in \part'} m(y\_B') \right)}{\crp(\part) \left( \prod\_{B \in \part} m(y\_B) \right)} \frac{1}{\crp(\part(x\_{1:n})|\part(x\_{1:n-1}))} \\\\
&=& \frac{m(y\_{B'\_0})}{m(y\_{B\_0})}.
\end{eqnarray}

Here $B\_0$ and $B'\_0$ is the block of customers before and after insertion of the new customer. Since $B'\_0 = B\_0 \cup \\{n\\}$, we get: $w\_n(x\_{1:n}) = \ell(y\_n | y\_{B\_0})$.

---

Another view of this algorithm is that we have a sequence of approximating measures $\pi^{(K)}\_n$, where approximation $\pi^{(K)}\_n$ is obtained recursively from $\pi^{(K)}\_{n-1}$ by randomly propagating each the atoms via $q(\cdot|\cdot)$, and updating the weights via $w\_n$. Since we have just rewritten the algorithm in an equivalent form it clearly keeps the asymptotic properties (in $K$) from last section.

One can view this as mass propagation/transportation on a tree organized by layers.

**Resampling:** Since we are often less interested in the previous measures $\pi\_{1}, \pi\_{2}, \dots$ and more interested in the latest one, $\pi\_{n}$ it is useful to trade a loss of information in the far past for a more accurate approximation near the present iteration. This is done via *resampling*.

The goal of resampling is to discard the particles of low weight while preserving the asymptotic guarantees outlined in the previous section.

In its simplest form (*multinomial resampling*), it consists in two steps:

1. Normalize the weights $w^{(1)}\_n, \dots, w^{(K)}\_n$ into $\bar w^{(1)}\_n, \dots, \bar w^{(K)}\_n$
2. Resample $K$ times from the categorical distribution with parameters $\bar w^{(1)}\_n, \dots, \bar w^{(K)}\_n$.

Let $\tilde x^{(1)}, \dots, \tilde x^{(K)}$ denote the list of particles sampled from step 2 above (note that there can be repeats). We construct the following new approximation from the $\tilde x^{(k)}$:

\begin{eqnarray}
\tilde \pi^{(K)}\_n = \sum\_k \frac{1}{K} \delta\_{\tilde x^{(k)}}.
\end{eqnarray}

Note that the weights have been all reset to $1/K$. Informally, this is because the random number of repeats now encodes $\bar w^{(K)}\_n$. Formally, it is a good exercise  to show that for all fixed $A$, 

\begin{eqnarray}
\E\left[\tilde \pi^{(K)}(A) | \pi^{(K)}\right] = \pi^{(K)}(A).
\end{eqnarray}

Using this and Minkowsky's inequality, it is not too hard to show that the resampling preserves the asymptotic guarantees as well (hint: prove convergence in L2). In particular, the normalization can now be consistently approximated by:

\begin{eqnarray}\label{eq:marg-smc}
Z^{(K)} = \prod\_n \frac{1}{K} w_n^{(k)}.
\end{eqnarray}

#### Particle MCMC (PMCMC)

Suppose we want to put a prior on and resampling $\alpha\_0$ and/or likelihood hyper-parameters $h$. Different values can induce potentially very different clusterings (especially for $h$). How can we change our SMC algorithm to handle such global variables $\theta = (\alpha\_0, h)$? This is where PMCMC comes in.

PMCMC methods are essentially MCMC algorithms in which SMC acts as a MH proposal. There is therefore an MCMC algorithm in the outer loop, indexed by $j$, and a SMC algorithm in the inner loop, with particle weights indexed by $w^{(j,k)}\_n$.

**Simplest PMCMC algorithm: PMMH**

Suppose first we only cared about getting a posterior over $\theta$. Let $p(\theta)$ denote its prior density. The obstacle is that $Z\_{\theta} = p(y | \theta)$, where $y$ is the data, is difficult to compute (recall that it would involves summing over all partitions $\part$ of the $n$ datapoints). On the other hand, for any fixed value of $\theta$, we can get an approximation of $Z\_{\theta} = p(y | \theta)$ provided by running the SMC algorithm of the previous section and returning Equation~(\ref{eq:marg-smc}).

Therefore, PMMH stores two quantities at each MCMC iteration $j$:

1. A current value of the global variable $\theta^{(j)}$.
2. An SMC estimate $Z^{(j)}$ corresponding to  $\theta^{(j)}$, i.e. an approximation of the marginal density of the data given $\theta^{(j)}$.

At each MCMC iteration $j$, PMMH takes these steps: 

1. It proposes a new value for the global variable, $\theta'$, using a proposal $q(\theta'|\theta)$ (note the abuse of notation, $q$ is also used for the very different proposal used within the SMC inner loop). 
2. It compute an approximation $Z'$ to the marginal density of the data given $\theta'$ using Equation~(\ref{eq:marg-smc}). 
3. It forms the ratio:
\begin{eqnarray}
r = \frac{p(\theta')}{p(\theta)} \frac{Z'}{Z^{(j-1)}} \frac{q(\theta|\theta')}{q(\theta'|\theta)}
\end{eqnarray}
3. Sample a Bernoulli$(\min(1,r))$,
   - If accepted, set $(\theta^{(j)}, Z^{(j)}) \gets (\theta', Z')$
   - Else, set $(\theta^{(j)}, Z^{(j)}) \gets (\theta^{(j-1)}, Z^{(j-1)})$
   
**Analysis:** 

For any fixed number of particles $K$, there will be an error in the approximation $Z^{(j)}$ of $Z\_{\theta'}$. It follows that the MH ratio will be different than the idealized ratio, 
\begin{eqnarray}
r^\star = \frac{p(\theta')}{p(\theta)} \frac{p(y|\theta')}{p(y|\theta)} \frac{q(\theta|\theta')}{q(\theta'|\theta)},
\end{eqnarray}
and it is therefore not obvious a priori what is the asympotic behavior of PMMH if we let the number of MCMC iterations go to infinity for a fixed $K$.

It was therefore a surprise when [Andrieu, Doucet and Holenstein, 2010](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00736.x/abstract) showed that even for fixed $K$ this converges to the right target, $p(\theta | y)$.

The basic idea is to introduce auxiliary variables modelling each step of the SMC inner loop, namely:

- $K(N-1)$ discrete variables $d$ on $\\{1, \dots, K\\}$ representing all the indices from the resampling steps for customers $2, 3, \dots, N$.
- $KN$ variables representing the partial seating arrangements $s$ stored by the $K$ particles for customers $1, 2, 3, \dots, N$.
- One extra random variable $t$ uniformly distributed on the $K^N$ resampling trajectories $\\{1, 2, \dots, K\\} \times \dots \times \\{1, 2, \dots, K\\}$.

The auxiliary distribution we put on these random variables can be described with the following generative process:

1. pick a value for the trajectory random variable $t$. This determines $N-1$ of the variables in (1) at the same time.
2. fill in the $N$ variables $s$ corresponding to the sampled trajectory according to the target (intractable) distribution $p(z|\theta)$.
3. sample the rest of the variables $s,d$ according to the standard SMC algorithm (with the difference that the sampled trajectory is forced to be resampled at least once at each generation). Call $r$ this subset of $s,d$.

Appendix B1 of [Andrieu, Doucet and Holenstein, 2010](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00736.x/abstract) shows that PMMH can be interpreted as a standard MH algorithm on that expanded space.

This augmented construction also provides a second PMCMC algorithm called Particle Gibbs (PG). It corresponds to doing block sampling on the augmented space, with the sampling steps divided as follows:

- $t | \textrm{rest}$
- $r | \textrm{rest}$
- $\theta | \textrm{rest}$.

Again, see [Andrieu, Doucet and Holenstein, 2010](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00736.x/abstract) for detail.

#### Cases where exact inference is possible

In some specialized cases, it is possible to do much better than MCMC and SMC, and we can get the exact value of $Z$ in polynomial time. Even when the whole problem is intractable, it is sometimes possible and beneficial to use exact inference method within approximate ones, for example as a block sampling techniques.

Again, we will start by assuming a discrete latent space, but the ideas here extends to other settings.

**Background:** We assume we are given a measure with a normalization constant that would naively take an exponential time to compute. We assume that this measure is decomposed into a *factor graph*:

- Circles are variables we need to sum over
- Squares are *factors*/*potentials* in a product decomposition
- There is an edge if a factor needs to know the value of the connected variable

A factor/potential is simply a positive function. 

In the discrete case, this is implemented via a vector, matrix, or tensor.

**Observations:**

- This form of graph is related to directed graphical models, but with important differences. V structures in particular creates additional connections in factor graphs.
- We only need variables for the unobserved variables (why?).

**Computing the normalization of tree shaped factor graph:**

- If we had a single node with a single with a single variable, this is easy. With more than one variable, we can naively sum over all the variables but this is exponential.
- Instead, if we have bigger graphs, we just need to show that we can quickly perform two transformations such that:
  - One of the transforms produces a new factor graph $g'$ with one less factor than $g$, the other one produces a graph with one less variable.
  - Both transforms preserve the normalization, $\normof{g'} = \normof{g}$.
  
**First transform:** pointwise product. Removes two factors $f\_1$ and $f\_2$ and replace them by one new factor $f'$. Comes in two versions, depending on the arity of $f\_2$ (number of connections):

\begin{eqnarray}
f'(x) &=& f\_1(x) f\_2(x) \\\\
f'(x,y) &=& f\_1(x) f\_2(x,y).
\end{eqnarray}

**Second transform:** marginalization. Removes one variable $x$ and replace a binary factor $f$ into a unary one $f'$.

\begin{eqnarray}
f'(y) = \sum\_x f(x,y).
\end{eqnarray}

**Optional exercises:** 

- Show that these two transforms preserve the normalization.
- Show that these two transforms can be used to change an arbitrary tree-shaped factor graph into a factor graph with a single variable and a single factor.
- Show how you can compute not only normalizations but also marginals over a single variable using the same algorithm. The only difference is that you will return the last factor, normalized.
- Show that you can modify the above algorithm to return all the one-node marginals by running the elimination along both directions of each edge.
- Show how you can modify this algorithms for the case where the potentials are multivariate normal distributions.


### What is next?

Here is an overview of some other active topics (each with a review/representative/recent paper) in the Bayesian non-parametric literature. We may spend more time on one or two of these, depending on the level of interest. Let me know if you think one of these will be particularly useful for your research.

- [Infinite Hidden Markov Models](http://www.cs.berkeley.edu/~jordan/papers/stickyHDPHMM_LIDS_TR.pdf): A stick breaking process gives us prior over a generalized multinomial. Now, imagine we view each of these as the row of a matrix and we generate an infinite list of rows (via a HDP). What do we get?
- [Dependent Dirichlet Processes](http://www2.warwick.ac.uk/fac/sci/statistics/staff/academic-research/steel/steel_homepage/techrep/ddp.pdf): Suppose now that we want a transient process. We can do this for example by introducing new sticks over time.
- [Indian Buffet Process](http://ai.stanford.edu/~tadayuki/papers/miller-phd-dissertation11.pdf): So far, we have assumed that each customer picks a single table/parameter. It is sometimes useful for each customer to be able to pick a random set of parameters instead.
- [Normalized Random Measures](http://www.stats.ox.ac.uk/~teh/research/npbayes/FavTeh2013a.pdf): Just as PY processes generalize DP, NRMs provide another route for generalizing DPs, with new efficient samplers being developed.
- [Gamma-exponential process](http://books.nips.cc/papers/files/nips24/NIPS2011_1162.pdf): A prior over infinite rate matrices.
- [Fragmentation-Coagulation](http://www.stats.ox.ac.uk/~teh/research/npbayes/TehEllBlu2013a.pdf)
- [Random graphs](http://arxiv.org/abs/1401.1137)


### Supplementary references and notes

**Under construction**
