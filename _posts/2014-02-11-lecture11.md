---
layout: post
title: "Lecture 11: Diffusions"
category: 'Lecture'
---

Instructor: Alexandre Bouchard-C&ocirc;t&eacute;

Editor: TBA


### Wrapping up phylogenetics


    
**Loss function:** if the goal is to reconstruct the tree, a standard loss function consists in counting the number of clades that disagree when comparing the true to the reconstructed one:

\begin{eqnarray}
\sum\_{c \subset \\{1, 2, \dots, n\\}} \1[\1(c \in t) = \1(c \in t')],
\end{eqnarray}

where we use the fact that the clades $c$ correspond to subsets of the set of leaves $\\{1, 2, \dots, n\\}$.

**Bayes estimator:** for the loss function given above, the Bayes estimator consists in returning the tree consisting in the clades $c$ with posterior probability greater than 1/2, $\\{c : \P(c \in T | Y) > 1/2\\}$. It can be shown that there always exists a tree $t^*$ that contains all the consensus clades (see [Semple and Steel, 2003](http://ukcatalogue.oup.com/product/9780198509424.do)).

**MCMC proposal:**  we can easily build an MCMC chain that explores the space of trees. We only need the following two proposals to have an irreducible chain (exercise):

1. propose a new branch, for example from the exponential prior,
2. propose a small perturbation on the tree (picture).

Note: in practice, a combination of other proposals are used to accelerate mixing, see for example [Lakner et al., 2008](http://www.nrm.se/download/18.42129f1312d951207af800041603/1367705004850/Lakner_et_al_SystBio_2008.pdf). Note that special proposals are also needed for ultrametric tree inference [Drummond et al., 2012](http://mbe.oxfordjournals.org/content/early/2012/02/25/molbev.mss075.abstract).

**MH ratio calculation:** once a new tree $t'$ has been proposed by some proposal $q$, we need to do an accept/reject step. The ratio is given by:

\begin{eqnarray}
\frac{p(t')}{p(t)} \frac{\ell(y | t')}{\ell(y | t)} \frac{q(t | t')}{q(t' | t)}.
\end{eqnarray}

While $p(\cdot)$ and $q(\cdot | \cdot)$ are typically easy (consider for example the proposals 1 and 2 above), $\ell(y | \cdot)$ needs some thoughts.

---

**Exercise:** show how to efficiently and exactly compute $\ell(y | \cdot)$.

Once you have established how to compute $\ell(y | \cdot)$ in the basic model outlined above, see if you can extend your method to more advanced phylogenetic models where:

1. each site is allowed to evolve at a different (iid latent, discretized) rate multiplying all the rate matrices at that site,
2. same as above, but where the latent rate categories are Markov,
3. codon models: where correlations between triplets of coding nucleotides are taken into account
4. covarion models, where the rate can change along the tree

---

### Diffusions: overview (not comprehensive) and some pointers

**Diffusion:** a continuous time, continuous space stochastic process with continuous paths.

#### Review: Brownian motion

**Definition:** A collection of random variables $Y_s(\omega)\in\RR, s\in\RR$ such that:

1. For all,  $s,t \in \RR$, the **increments** are normally distributed:
\\begin{eqnarray}
(Y\_{s+t} - Y\_s) \sim N(0,\sigma^2 t)
\\end{eqnarray}
2. For any two disjoint intervals, $(t\_1, t\_2]$ and $(t\_3, t\_4]$, the increments $(Y\_{t\_2} - Y\_{t\_1})$ and $(Y\_{t\_4} - Y\_{t\_3})$ are independent.
3. The process starts at zero ($Y\_0 = 0$) and its paths are continuous functions of $s$.

**Practical question:** How to sample from a Brownian motion? Example: using Monte Carlo, approximate the probability that a Brownian motion crosses the line $y=+1$ at least once in the interval $[0, 1/2]$.

**Important idea:** Lazy computing. Often, the question we try to answer does not require the knowledge of all points $Y\_s$ (or at least, can be approximated using a finite number of points). Only instantiate the value at those points $s\_i$ that matter.

**Note:** we may not know a priori what points matter. For example, with the above question may want to dynamically refine the approximation in a region very close to the line $y = +1$.

**Refining the Brownian motion FDDs**: Let's say we have only sampled $Y\_{0.1}$ and $Y\_{0.3}$. We want to add a sample $Y\_{0.2}$ after the fact. How can we do this?

- Note: it can be shown that by characterization 1-3 above, $(Y\_{0.1}, Y\_{0.2}, Y\_{0.3})$ has to be Multivariate Normal.
- Mean: $(0,0,0)$.
- Covariance: say $0 < s < t$, we have:
\\begin{eqnarray}
\\Cov(Y\_s, Y\_t) & = & \E[Y\_s Y\_t] \\\\
             & = & \E[Y\_s (Y\_t - Y\_s + Y\_s)] \\\\
             & = & \E[(Y\_s)^2] + \E[Y\_s - Y\_0] \E[Y\_t - Y\_s] \\\\
             & = & \sigma^2 s + 0. 
\\end{eqnarray}
- Therefore, we can sample from the conditional distribution, $Y\_{0.2} | (Y\_{0.1}, Y\_{0.3})$, which is also Normal&mdash;see Rasmussen's book on Gaussian processes, available online [here](http://www.gaussianprocess.org/gpml/chapters/RWA.pdf).

#### Kalman filter

Suppose that a Brownian motion (possibly multivariate) is not observed directly, but rather with normal distributed errors. 

\\begin{eqnarray}
Z\_t &\sim& \textrm{BM}(\sigma\_1^2) \\\\
Y\_{t\_i} | Z\_{t\_i} &\sim& \Norm(Z\_{t\_i}, \sigma\_2^2),
\\end{eqnarray}

where $\sigma\_1^2, \sigma\_2^2$ are parameters, considered fixed for now.

Our goal is to represent the posterior over $n$ latent FDDs (potentially jointly with other queries) given the observations.

Since each component of the model is normal, the model is multivariate normal, so in theory this could be done with a multivariate conditional normal calculation via the [Shur complement](http://en.wikipedia.org/wiki/Schur_complement).  The major drawback of this method is that it requires inverting an $n$ by $n$ matrix, a costly operation.

Instead, one can use the factor graph framework, where each factor encodes a normal distribution, plus a normalization. The normal is univariate for unary factors, and bivariate for binary factors. 

Using the closure of the multivariate normal family under marginalization, we get that the variable elimination steps will produce a factor of the assumed type. Similarly, pointwise product of normal densities will also produce an unnormalized normal density.

The factor graph framework can therefore be used to reduce the computation to a cost linear in $n$, a considerable gain in applications where we may encounter long time series.

Note that this can be used on a tree as well, for example for allele drifts among tree-related populations (see [Felsenstein (2004)](http://www.amazon.com/Inferring-Phylogenies-Joseph-Felsenstein/dp/0878931775), chapter 23 for an accessible exposition).

#### Extension: Gaussian Process

**Motivation:** going from random function with $\RR \to \RR$ realization to random function with $\Xscr \to \RR$, where assumptions on $\Xscr$ are weak (more below). 

**Examples:** 

- spatial statistics (prior over temperature field), 
- meta-modelling (performance on cross-validation of a black-box for various hyper-parameter settings),
- general regression problems,
- classification via a GLM construction.

**Construction:**

1. We want the FDDs $(X\_{s\_1}, \dots, X\_{s\_n}, s\_i \in \Xscr)$ to be multivariate normal, let us say with mean zero.
2. All we need is therefore a way of getting a covariance matrix from any finite list of locations ${s\_1}, \dots, {s\_n}, s\_i \in \Xscr$.
3. Continuity requirement, combined with the theory of Reproducing kernel Hilbert spaces tell us that the general way of getting covariance matrices in (2) is as follows:
  - construct a suitable function $k$ on pairs of points, called a **covariance function** $k : \Xscr \times \Xscr \to [0, \infty)$ (important requirements on that function to be specified shortly).
  - build the covariance matrix by evaluating $k$ on all pairs $K = (k(s\_i, s\_j))\_{i,j}$.

**Covariance function:** not all functions $k$ will allow us to go through the above process. The key constraint is that we should have the guarantee that we get a non-negative matrix $K$ no matter what points ${s\_1}, \dots, {s\_n}, s\_i \in \Xscr$ we use as inputs. This means:

- Clearly, the function should always be symmetric.
- Having all the entries positive is sufficient, but not necessary.
- The general condition is non-negative definiteness ($\int \int k\; \ud \mu \times \ud \mu \ge 0$ for all $\mu$). Several equivalent characterizations exist. For example, that $k$ should be a dot product in an augmented space. 

**Examples:**

- Squared exponential: $\exp(- r^2 / 2\ell^2)$, for $r = |x - x'|$,
- Matern: 

\begin{eqnarray}
\frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu}r}{\ell}\right)^\nu K\_nu\left(\frac{\sqrt{2\nu}r}{\ell}\right),
\end{eqnarray}

where $K\_\nu$ is the [modified Bessel function](http://en.wikipedia.org/wiki/Bessel_function#Modified_Bessel_functions_:_I.CE.B1.2C_K.CE.B1).

See [Rasmussen and Williams (2006)](http://www.gaussianprocess.org/gpml/chapters/RW4.pdf), chapter 4 for more characterizations and examples.

**Notes:**

- The diagonal value of the covariance function $(k(s,s))$ can be interpreted as observation noise. 
- The case $k(s,s) = 0$ is permitted (and used extensively in meta-modelling applications)
- The other values of $k$ can be interpreted as marginal covariances of pairs of points.

**Computations:** 

A large field exists on alleviating the cubic computational burden. See these [references](http://www.gaussianprocess.org/#references). General overview:

- One way to reduce the cost is to have a sparse precision (inverse covariance) matrix. In fact, with enough zeros, it becomes possible to use the factor graph algorithm from the previous section to get an exact answer.
- Factor graph algorithms are only exact on trees, but they can be generalized to arbitrary graphs (and covariance matrices). See the references on EP in the above resources. 
- The above approach falls under the framework of variational inference. Another recent, popular GP approximation approach is that of [Rue, Martino and Chopin, 2009](http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2008.00700.x/abstract).
- Another popular approach is the Nystrom approximation, see for example [Quinonero-Candela and Rasmussen, 2005](http://www.quinonero.net/Publications/quinonero-candela05a.pdf).

#### Other diffusions

**Under construction**

### Supplementary references and notes

**Under construction**
